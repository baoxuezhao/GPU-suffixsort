#include "sufsort_kernel.h"
#include "../Timer.h"
#include "globalscan.cu"
/*
 * thrust header
 */
#include <thrust/scan.h>
#include <thrust/device_ptr.h>
#include <thrust/sort.h>
#include <thrust/scatter.h>
#include <thrust/gather.h>

/**
 * GPU sort header
 *
 */
//#include <b40c/util/error_utils.cuh>
//#include <b40c/util/multiple_buffering.cuh>
//#include <b40c/radix_sort/enactor.cuh>
//#include <b40c_test_util.h>

__constant__ __device__ uint32 round_pow2[MIN_UNSORTED_GROUP_SIZE+2];

__device__ __forceinline__ bool less_than(uint32 *isa, uint32 h, uint32 a, uint32 b)
{
	if (a == b)
		return false;
	while (true)
	{
		if (isa[a] < isa[b])
			return true;
		else if (isa[a] > isa[b]) 
			return false;
		else
		{
			a += h;
			b += h;
		}
	}
}


__device__ __forceinline__ bool less_than_init(uint32 *a, uint32 *b, uint4 a_value, uint4 b_value)
{
	
	if (a_value.x != b_value.x)
		return a_value.x < b_value.x;
	if (a_value.y != b_value.y)
		return a_value.y < b_value.y;
	if (a_value.z != b_value.z)
		return a_value.z < b_value.z;
	if (a_value.w != b_value.w)
		return a_value.w < b_value.w;
	
	uint4 av;
	uint4 bv;

	while (true)
	{
		av.x = *a;
		av.y = *(a+1);
		av.z = *(a+2);
		av.w = *(a+3);

		bv.x = *b;
		bv.y = *(b+1);
		bv.z = *(b+2);
		bv.w = *(b+3);
		
		if (av.x != bv.x)
			return av.x < bv.x;
		if (av.y != bv.y)
			return av.y < bv.y;
		if (av.z != bv.z)
			return av.z < bv.z;
		if (av.w != bv.w)
			return av.w < bv.w;
		a+=4;
		b+=4;

		av.x = *a;
		av.y = *(a+1);
		av.z = *(a+2);
		av.w = *(a+3);

		bv.x = *b;
		bv.y = *(b+1);
		bv.z = *(b+2);
		bv.w = *(b+3);

		if (av.x != bv.x)
			return av.x < bv.x;
		if (av.y != bv.y)
			return av.y < bv.y;
		if (av.z != bv.z)
			return av.z < bv.z;
		if (av.w != bv.w)
			return av.w < bv.w;
		a+=4;
		b+=4;

		av.x = *a;
		av.y = *(a+1);
		av.z = *(a+2);
		av.w = *(a+3);

		bv.x = *b;
		bv.y = *(b+1);
		bv.z = *(b+2);
		bv.w = *(b+3);
		if (av.x != bv.x)
			return av.x < bv.x;
		if (av.y != bv.y)
			return av.y < bv.y;
		if (av.z != bv.z)
			return av.z < bv.z;
		if (av.w != bv.w)
			return av.w < bv.w;
		a+=4;
		b+=4;
	}
}

__device__ __forceinline__ void swap(volatile uint32 &a, volatile uint32 &b)
{
	uint32 tmp = a;
	a = b;
	b  = tmp;
}

/*
__global__ void reset_round_pow2()
{
	uint32 tid = threadIdx.x;
	uint32 val = (1<<(uint32)(__log2f(tid)));
	if (val < tid)
		val = (val<<1);
	round_pow2[tid] = val;
	if (tid == 0)
	{	
		round_pow2[MIN_UNSORTED_GROUP_SIZE] = MIN_UNSORTED_GROUP_SIZE;
		for (uint32 i = 1; i <= 256; i++)
			printf("%u: %u\n", i, round_pow2[i]);
	}
}
*/

__global__ void bitonic_sort_kernel_init(uint32 *d_len, uint32 *d_start, uint32 *d_sa, uint32 *d_misa, uint32 h, uint32 num_interval, uint32 size, uint32 module, uint32 global_num)
{
	uint32 interval_start = blockIdx.x*num_interval;
	uint32 interval_end = interval_start + num_interval;
	if (interval_end > size)interval_end = size;
	
	volatile __shared__ uint32 shared_key[MIN_UNSORTED_GROUP_SIZE];
	volatile __shared__ uint4 shared_value[MIN_UNSORTED_GROUP_SIZE];
	uint32 tid = threadIdx.x;
	uint32 tid_key;
	uint32* tid_misa;
	uint4 tid_data;
	for (uint32 i = interval_start; i < interval_end; i++)
	{
		uint32 start = d_start[i];
		uint32 end = start + d_len[i];
		uint32 len = end-start;
		
		uint32 round2_len = 1<<(int)(__log2f(len));
		if (round2_len < len)
			round2_len = (round2_len<<1);

		if (tid+start < end)
		{	
			tid_key = shared_key[tid] = d_sa[tid+start];
			tid_misa = d_misa + ((tid_key&(module-1))*global_num + tid_key/module);
		
			tid_data.x = *tid_misa;
			tid_data.y = *(tid_misa+1);
			tid_data.z = *(tid_misa+2);
			tid_data.w = *(tid_misa+3);

			shared_value[tid].x = tid_data.x;
			shared_value[tid].y = tid_data.y;
			shared_value[tid].z = tid_data.z;
			shared_value[tid].w = tid_data.w;

		//	tid_data = shared_value[tid] = *(tid_misa);
		//	shared_value[tid].x = tid_data.x;
		//	shared_value[tid].y = tid_data.y;
		}
		if (len+tid < round2_len)	
		{	
			shared_key[len+tid] = 0;
		//	tid_data.x = shared_value[len+tid].x = 0xffffffffffffffff;	
			tid_data.x = shared_value[len+tid].x = 0xffffffff;
		}
		__syncthreads(); 
		
    		// Parallel bitonic sort.
    		for (uint k = 2; k <= round2_len; k *= 2)
    		{
        		// Bitonic merge:
        		for (int j = k/2; j > 0; j /= 2)
        		{
				if (tid < round2_len)
				{
					uint32 ixj = tid ^ j;
					uint32 ixj_key = shared_key[ixj];
					uint32 *ixj_misa = d_misa + ((ixj_key&(module-1))*global_num + ixj_key/module);
					
					if (ixj > tid)
					{
						if ((tid & k) == 0)
						{
							if (less_than_init(ixj_misa+4, tid_misa+4, shared_value[ixj], tid_data))
								swap(shared_key[tid], shared_key[ixj]);
						}
						else
						{
							if (less_than_init(tid_misa+4, ixj_misa+4, tid_data, shared_value[ixj]))
								swap(shared_key[tid], shared_key[ixj]);
						}
					}
         		   	}
           	 		__syncthreads();
        		}
    		}
		__syncthreads();

		// Write back the sorted data to its correct position
		if (start+tid < end)
			d_sa[start+tid] = shared_key[tid];
		__syncthreads();
	}
}

__global__ void bitonic_sort_kernel(uint32 *d_len, uint32 *d_start, uint32 *d_sa, uint32 *d_isa, uint32 h, uint32 num_interval, uint32 size, uint32 max_val)
{
	uint32 interval_start = blockIdx.x*num_interval;
	uint32 interval_end = interval_start + num_interval;
	if (interval_end > size)interval_end = size;
	
	__shared__ volatile uint32 shared_key[MIN_UNSORTED_GROUP_SIZE];
	
	for (uint32 i = interval_start; i < interval_end; i++)
	{
		uint32 start = d_start[i];
		uint32 end = start + d_len[i];
	//	uint32 round2_len = round_pow2[end-start];
		uint32 len = end-start;
		
		uint32 round2_len = 1<<(int)(__log2f(len));
		if (round2_len < len)
			round2_len = (round2_len<<1);

		if (threadIdx.x+start < end)
			shared_key[threadIdx.x] = d_sa[threadIdx.x+start];

		if (len+threadIdx.x < round2_len)	
			shared_key[len+threadIdx.x] = max_val;	
		__syncthreads(); 
		
    		// Parallel bitonic sort.
    		for (uint k = 2; k <= round2_len; k *= 2)
    		{
        		// Bitonic merge:
        		for (int j = k/2; j > 0; j /= 2)
        		{
				int tid = threadIdx.x;
				if (tid < round2_len)
				{
					unsigned int ixj = tid ^ j;
					if (ixj > tid)
					{
						if ((tid & k) == 0)
						{
							if (less_than(d_isa, h, shared_key[ixj], shared_key[tid]))
								swap(shared_key[tid], shared_key[ixj]);
						}
						else
						{
							if (less_than(d_isa, h, shared_key[tid], shared_key[ixj]))
								swap(shared_key[tid], shared_key[ixj]);
						}
					}
         		   	}
           	 		__syncthreads();
        		}
    		}
		__syncthreads();

		// Write back the sorted data to its correct position
		if (start+threadIdx.x < end)
			d_sa[start+threadIdx.x] = shared_key[threadIdx.x];
		__syncthreads();
	}
}

__global__ void gather_module_based_isa_kernel(uint32 *isa, uint32 *misa, uint32 module, uint32 num_interval, uint32 round_string_size, uint32 global_num)
{
	uint32 interval_start = blockIdx.x * num_interval;
	uint32 interval_end = interval_start + num_interval;
	uint32 tid = threadIdx.x;
	uint32 local_num = NUM_ELEMENT_SB/module;

	volatile __shared__ uint32 shared[NUM_ELEMENT_SB];
	volatile uint32 *shared_value = shared+tid;
	
	for (uint32 p = interval_start; p < interval_end; p++)
	{
		uint32 start = NUM_ELEMENT_SB*p;
		uint32 *global_value = isa + start + tid;

		shared_value[0] = global_value[0];
		shared_value[NUM_THREADS] = global_value[NUM_THREADS];
		shared_value[2*NUM_THREADS] = global_value[2*NUM_THREADS];
		shared_value[3*NUM_THREADS] = global_value[3*NUM_THREADS];
		shared_value[4*NUM_THREADS] = global_value[4*NUM_THREADS];
		shared_value[5*NUM_THREADS] = global_value[5*NUM_THREADS];
		shared_value[6*NUM_THREADS] = global_value[6*NUM_THREADS];
		shared_value[7*NUM_THREADS] = global_value[7*NUM_THREADS];

		__syncthreads();

		uint32 index = tid*module;
		uint32 *misa_ptr = misa + p*local_num + tid;
		misa_ptr[0] = shared[index];
		misa_ptr[global_num] = shared[index+1];
		misa_ptr[2*global_num] = shared[index+2];
		misa_ptr[3*global_num] = shared[index+3];
	
		index += NUM_ELEMENT_SB/2;
		misa_ptr += NUM_ELEMENT_SB/8;
		misa_ptr[0] = shared[index];
		misa_ptr[global_num] = shared[index+1];
		misa_ptr[2*global_num] = shared[index+2];
		misa_ptr[3*global_num] = shared[index+3];

		__syncthreads();
	}
}

__global__ void scatter_small_group_kernel(uint32 *d_block_len, uint32 *d_block_start, uint32 *d_sa,
	 uint32 *d_isa, uint32 num_interval, uint32 size)
{
	uint32 interval_start = blockIdx.x * num_interval;
	uint32 interval_end = interval_start + num_interval;
	uint32 tid = threadIdx.x + interval_start;

	if (interval_end > size)interval_end = size;
	
	uint32 start, end, i, j;
	
	#pragma unroll
	for (i = tid; i < interval_end; i += THREADS_PER_BLOCK)
	{
		start = d_block_start[i];
		end = d_block_len[i]+start;
		for (j = start; j < end; j++)
			d_isa[d_sa[j]] = j+1;
	}
}

__global__ void scatter_large_group_kernel(uint32 *d_block_len, uint32 *d_block_start, uint32 *d_sa, 
	uint32 *d_isa, uint32 num_interval, uint32 size)
{
	uint32 interval_start = blockIdx.x * num_interval;
	uint32 interval_end = interval_start + num_interval;
	uint32 tid = threadIdx.x + interval_start;

	if (interval_end > size)interval_end = size;

	uint32 start, end, i, j, value;

	#pragma unroll
	for (i = tid; i < interval_end; i += THREADS_PER_BLOCK)
	{
		value = start = d_block_start[i];
		end = d_block_len[i]+start;
		value++;
		for (j = start; j < end; j++)
			d_isa[d_sa[j]] = value;
	}
}

__global__ void update_block_kernel1_init(uint32 *ps_array, uint32 *d_value, 
	Partition *d_par, uint32 num_interval, uint32 par_count)
{	
	uint32 interval_start = blockIdx.x*num_interval;
	uint32 interval_end = interval_start + num_interval;
	uint32 tid = threadIdx.x;
	
	__shared__ volatile uint32 shared[NUM_ELEMENT_SB+32];
	
	if (interval_end > par_count)interval_end = par_count;
	
	#pragma unroll
	for (uint i = interval_start; i < interval_end; i++)
	{
		uint32 start = d_par[i].start;
		uint32 end = d_par[i].end;
		uint32 bid = d_par[i].bid;
		
		#pragma unroll
		for (uint32 index = start + tid; index < end; index += THREADS_PER_BLOCK)
			shared[index-start] = ps_array[index];
	
		__syncthreads();	

		if (tid == 0)
		{	
			
			//	the last element of previous interval
			if (bid || ps_array[d_par[i-1].end-1] != shared[0])
				d_value[shared[0]] = start;

			#pragma unroll
			for (uint32 index = start + THREADS_PER_BLOCK; index < end; index += THREADS_PER_BLOCK)
			{
				uint32 index1 = index - start;
			//	if (shared[index1] != shared[index1-1])
				if (ps_array[index] != ps_array[index-1])
					d_value[shared[index1]] = index;
			}
		}
		else
		{
						
			#pragma unroll
			for (uint32 index = start + tid; index < end; index += THREADS_PER_BLOCK)
			{
				uint32 index1 = index - start;
				if (shared[index1] != shared[index1-1])
					d_value[shared[index1]] = index;
			}
		}
	}
}

__global__ void update_block_kernel1(uint32 *ps_array, uint32 *d_len, uint32 *d_value, uint32 *d_block_start, uint32 *d_block_len, uint32 block_count)
{	
	uint32 tid = threadIdx.x;
	uint32 block = blockIdx.x;

	//shared memory will be used later
//	__shared__ volatile uint32 shared[NUM_ELEMENT_SB+32];
	
	#pragma unroll
	for (uint p = block; p < block_count; p += BLOCK_NUM)
	{
		uint32 start = d_block_start[p];
		uint32 end = start + d_block_len[p];

		for (uint32 i = tid+start; i < end-1; i += THREADS_PER_BLOCK)
			if (ps_array[i] != ps_array[i+1])
				d_value[ps_array[i+1]] = i+1;
		if (tid == 0)
			d_value[ps_array[start]] = start;
		uint32 offset_start = ps_array[start];

		__syncthreads();

		for (uint32 i = tid; i < ps_array[end-1]-offset_start; i += THREADS_PER_BLOCK)
			d_len[i+offset_start] = d_value[i+offset_start+1] - d_value[i+offset_start];
		if (tid == 0)
		{
			if (ps_array[end-1] == ps_array[end-2])
				d_len[ps_array[end-1]] = end-d_value[ps_array[end-1]];
			else
				d_len[ps_array[end-1]] = 1;
		}
	}
}



__global__ void update_block_kernel2_init(uint32 *d_keys, uint32 *d_values, uint32 size)
{
	uint32 start = (blockIdx.x*blockDim.x)<<2;
	uint32 tid = start + threadIdx.x;
	uint32 bound = (blockIdx.x+1)*(blockDim.x<<2);
	if (bound > size)
		bound = size+1;

	__shared__ volatile uint32 shared[NUM_ELEMENT_SB+32];
	
	#pragma unroll
	for (uint32 index = tid; index < bound; index += THREADS_PER_BLOCK)
		shared[index-start] = d_values[index];
	

	if (threadIdx.x == THREADS_PER_BLOCK-1)
		shared[bound-start] = d_values[bound];
	__syncthreads();

	#pragma unroll
	for (uint32 index = tid; index < bound; index += THREADS_PER_BLOCK)
		d_keys[index] = shared[index+1-start] - shared[index-start];
}

//TODO: to be optimized later
__global__ void update_block_kernel2(uint32 *d_keys, uint32 *d_values, uint32 size)
{
	uint32 start = (blockIdx.x*blockDim.x)<<2;
	uint32 tid = start + threadIdx.x;
	uint32 bound = (blockIdx.x+1)*(blockDim.x<<2);
	if (bound > size)
		bound = size+1;

	__shared__ volatile uint32 shared[NUM_ELEMENT_SB+32];
	
	#pragma unroll
	for (uint32 index = tid; index < bound; index += THREADS_PER_BLOCK)
		shared[index-start] = d_values[index];

	if (threadIdx.x == THREADS_PER_BLOCK-1)
		shared[bound-start] = d_values[bound];
	__syncthreads();

	#pragma unroll
	for (uint32 index = tid; index < bound; index += THREADS_PER_BLOCK)
	{	
	//	d_keys[index] = (!((0x80000000&shared[index-start])-(0x80000000&shared[index+1-start])))*(shared[index+1-start] - shared[index-start]);

		if (shared[index+1-start] >= (1<<31))
			d_keys[index] = (0x7fffffff&shared[index+1-start]) - shared[index-start];
		else if (shared[index-start] >= (1<<31))
			d_keys[index] = 0;
		else
			d_keys[index] = shared[index+1-start] - shared[index-start];
		if (d_keys[index] >= (1<<31))
			printf("error: %u %u\n", d_values[index], d_values[index+1]);
	}
}

__global__ void find_boundary_kernel_init(uint32 *d_len, uint32 size)
{
	uint32 start = (blockIdx.x*blockDim.x) << 2;
	uint32 tid = start + threadIdx.x;
	uint32 bound = (blockIdx.x+1)*(blockDim.x<<2);
	
	if (blockIdx.x == 0 && threadIdx.x == 0)
		d_len[size+1] = 0;

	if (bound >= size)
		bound = size-1;

	__shared__ volatile uint32 shared[NUM_ELEMENT_SB + 32];

	for (uint32 index = tid; index < bound; index += THREADS_PER_BLOCK)
		shared[index-start] = d_len[index];

	if (threadIdx.x == THREADS_PER_BLOCK-1)
		shared[bound-start] = d_len[bound];

	__syncthreads();
	
	for (uint32 index = tid-start; index < bound-start; index += THREADS_PER_BLOCK)
	{	
		if (shared[index] <= MIN_UNSORTED_GROUP_SIZE && shared[index+1] > MIN_UNSORTED_GROUP_SIZE)
				d_len[size+1] = index+start+1;
		else if (shared[index] == 1 && shared[index+1] > 1)
				d_len[size+2] = index+start+1;
		else if (shared[index] <= MAX_SEG_NUM && shared[index+1] > MAX_SEG_NUM)
				d_len[size+3] = index+start+1;
	}	
}


__global__ void find_boundary_kernel(uint32 *d_len, uint32 size)
{
	uint32 start = (blockIdx.x*blockDim.x) << 2;
	uint32 tid = start + threadIdx.x;
	uint32 bound = (blockIdx.x+1)*(blockDim.x<<2);

	if (bound > size)
		bound = size;
	__shared__ volatile uint32 shared[NUM_ELEMENT_SB + 32];

	for (uint32 index = tid; index < bound; index += THREADS_PER_BLOCK)
		shared[index-start] = d_len[index];

	if (threadIdx.x == THREADS_PER_BLOCK-1)
		shared[bound-start] = d_len[bound];

	__syncthreads();
	
	/**
	 * d_len[size]: bsort_boundary
	 * d_len[size+1]: greater than one boundary
	 * d_len[size+2]: l type boundary
	 * d_len[size+3]: greater than zero boundary
	 */
	for (uint32 index = tid-start; index < bound-start; index += THREADS_PER_BLOCK)
	{	
		if (shared[index] <= MIN_UNSORTED_GROUP_SIZE && shared[index+1] > MIN_UNSORTED_GROUP_SIZE)
			d_len[size] = index+start+1;
		else if (shared[index] == 0 && shared[index+1] == 1)
			d_len[size+3] = index+start+1;
		if (shared[index] == 1 && shared[index+1] > 1)
			d_len[size+1] = index+start+1;
		if (shared[index] <= MAX_SEG_NUM && shared[index+1] > MAX_SEG_NUM)
			d_len[size+2] = index+start+1;
	}	
	
	if (blockIdx.x == 0 && threadIdx.x == 0)
		if (d_len[size] <= MIN_UNSORTED_GROUP_SIZE)
			d_len[size] = size;
}

__global__ void scatter_kernel(uint32 *d_L, uint32 *d_R_in, uint32 *d_R_out, Partition *d_par)
{
	/*
	 * use shift instead of multiplication(times 4)
	 */
	uint32 start = d_par[blockIdx.x].start;
	uint32 end = d_par[blockIdx.x].end;
	uint32 tid = start + (threadIdx.x<<2);
	
	if (tid >= end-start)
		return;
	
	extern __shared__ uint4 shared_memory[];

	uint4 *R_in = shared_memory;
	uint4 *L = shared_memory + THREADS_PER_BLOCK;

	R_in[threadIdx.x] = *((uint4*)(d_R_in+tid));
	L[threadIdx.x] = *((uint4*)(d_L+tid));

	d_R_out[L[threadIdx.x].x] = R_in[threadIdx.x].x;
	d_R_out[L[threadIdx.x].y] = R_in[threadIdx.x].y;
	d_R_out[L[threadIdx.x].z] = R_in[threadIdx.x].z;
	d_R_out[L[threadIdx.x].w] = R_in[threadIdx.x].w;
}

__global__ void generate_bucket_with_shift(uint32 *d_sa, uint32 *d_ref, uint32 *d_isa, uint32 string_size)
{
	uint32 tid = (blockIdx.x*blockDim.x + threadIdx.x);
	uint32 tid4 = tid*4;
	uint32 cur_block, next_block;
	
	//boundary check 
	if (tid4 > string_size)
		return;
	
	extern __shared__ uint32 segment_ref[];
	
	d_sa[tid4] = tid4;
	d_sa[tid4+1] = tid4+1;
	d_sa[tid4+2] = tid4+2;
	d_sa[tid4+3] = tid4+3;
	
	cur_block = d_ref[tid];
	
	//change from little-endian to big-endian
	cur_block = ((cur_block<<24)|((cur_block&0xff00)<<8)|((cur_block&0xff0000)>>8)|(cur_block>>24));
	segment_ref[threadIdx.x] = cur_block;

	if (threadIdx.x == THREADS_PER_BLOCK-1)
	{	
		next_block = d_ref[tid+1];
		
		//change from little-endian to big-endian
		next_block = ((next_block<<24)|((next_block&0xff00)<<8)|((next_block&0xff0000)>>8)|(next_block>>24));

		segment_ref[threadIdx.x+1] = next_block;
	}
	__syncthreads();
	
	next_block = segment_ref[threadIdx.x+1];
	
	/*
	 *  shift operation on little endian system
	 */
	d_isa[tid4] = cur_block;
	d_isa[tid4+1] = ((cur_block << 8) | (next_block >> 24));
	d_isa[tid4+2] = ((cur_block << 16) | (next_block >> 16));
	d_isa[tid4+3] = ((cur_block << 24) | (next_block >> 8));
}

__global__ void generate_bucket_without_shift(uint32 *d_sa, uint8 *d_ref, uint32 *d_isa, uint32 string_size)
{
	uint32 tid = (blockIdx.x*blockDim.x + threadIdx.x);
	uint32 tid4 = tid*4;
	
	//boundary check 
	if (tid4 >= string_size)
		return;
	
	d_sa[tid4] = tid4;
	d_sa[tid4+1] = tid4+1;
	d_sa[tid4+2] = tid4+2;
	d_sa[tid4+3] = tid4+3;
	
	uint8* local_d_ref = d_ref+tid4;

	d_isa[tid4] = ((uint32*)local_d_ref)[tid];
	d_isa[tid4+1] = ((uint32*)(local_d_ref+1))[tid];
	d_isa[tid4+2] = ((uint32*)(local_d_ref+2))[tid];
	d_isa[tid4+3] = ((uint32*)(local_d_ref+3))[tid];
}

__global__ void get_second_keys(uint32 *d_sa, uint32 *d_isa_in, uint32 *d_isa_out, Partition *d_par, uint32 num_interval, uint32 par_count)
{
	uint32 interval_start = blockIdx.x * num_interval;
	uint32 interval_end = interval_start + num_interval;
	uint32 tid = threadIdx.x;
	uint32 start, end;
	
	if (interval_end > par_count)interval_end = par_count;
	
	#pragma unroll
	for (uint32 i = interval_start; i < interval_end; i++)
	{
		start = d_par[i].start;
		end = d_par[i].end;
		for (uint32 index = tid+start; index < end; index += THREADS_PER_BLOCK)
			d_isa_out[index] = d_isa_in[d_sa[index]];
	}
/*
	extern __shared__ uint4 shared_memory[];

	uint4* segment_sa = shared_memory;
	uint4* out = shared_memory + THREADS_PER_BLOCK;
	uint4* d_out_ptr = (uint4*)(d_isa_out+tid);

	segment_sa[threadIdx.x] = *((uint4*)(d_sa+tid));
	
	if (segment_sa[threadIdx.x].x < h_boundary)
		out[threadIdx.x].x = d_isa_in[segment_sa[threadIdx.x].x];

	if (segment_sa[threadIdx.x].y < h_boundary)
		out[threadIdx.x].y = d_isa_in[segment_sa[threadIdx.x].y];

	if (segment_sa[threadIdx.x].z < h_boundary)
		out[threadIdx.x].z = d_isa_in[segment_sa[threadIdx.x].z];

	if (segment_sa[threadIdx.x].w < h_boundary)
		out[threadIdx.x].w = d_isa_in[segment_sa[threadIdx.x].w];

	*d_out_ptr = out[threadIdx.x];	
*/
		
}

__global__ void get_first_keys(uint32 *d_sa, uint32 *d_isa_in, uint32 *d_isa_out, uint32 size)
{
	/*
	 * use shift instead of multiplication (multiply by 4)
	 */
	uint32 tid = ((blockIdx.x*blockDim.x + threadIdx.x) << 2);

	if (tid >= size)
		return;

	extern __shared__ uint4 shared_memory[];
	
	uint4* out = shared_memory;
	uint4* segment_sa4 = shared_memory + THREADS_PER_BLOCK;
	uint4* d_isa_out_ptr = (uint4*)(d_isa_out+tid);
	
	segment_sa4[threadIdx.x] = *((uint4*)(d_sa+tid));

	
	out[threadIdx.x].x = d_isa_in[segment_sa4[threadIdx.x].x];
	out[threadIdx.x].y = d_isa_in[segment_sa4[threadIdx.x].y];
	out[threadIdx.x].z = d_isa_in[segment_sa4[threadIdx.x].z];
	out[threadIdx.x].w = d_isa_in[segment_sa4[threadIdx.x].w];

	*d_isa_out_ptr = out[threadIdx.x];
}


/**
 * TODO:There're two ways to handle memory access, try another one later
 * TODO:need to use templete to allocate register files, a bug of nvcc reported by moderngpu 
 */
__global__ void neighbour_comparison_kernel(uint32 *d_keys, uint32 *d_output, 
		Partition *d_par, uint32 num_interval, uint32 size)
{

	uint32 interval_start = blockIdx.x*num_interval;
	uint32 interval_end = interval_start + num_interval;
	uint32 tid = threadIdx.x;
	
	__shared__ volatile uint32 shared[NUM_ELEMENT_SB+32];
	
	if (interval_end > size)interval_end = size;
	
	#pragma unroll
	for (uint i = interval_start; i < interval_end; i++)
	{
		uint32 start = d_par[i].start;
		uint32 end = d_par[i].end;
		uint32 bid = d_par[i].bid;

		#pragma unroll
		for (uint32 index = start + tid; index < end; index += THREADS_PER_BLOCK)
			shared[index-start] = d_keys[index];
	
		__syncthreads();	

		if (tid == 0)
		{	
			//	the last element of previous interval
			if (bid || d_keys[d_par[i-1].end-1] != shared[0])
				d_output[start] = 1;
			else
				d_output[start] = 0;
			
			#pragma unroll
			for (uint32 index = start + THREADS_PER_BLOCK; index < end; index += THREADS_PER_BLOCK)
			{
				uint32 index1 = index - start;
			//	if (shared[index1] != shared[index1-1])
				if (d_keys[index] == d_keys[index-1])
					d_output[index] = 0;
				else
					d_output[index] = 1;
			}
		}
		else
		{
						
			#pragma unroll
			for (uint32 index = start + tid; index < end; index += THREADS_PER_BLOCK)
			{
				uint32 index1 = index - start;
				if (shared[index1] == shared[index1-1])
					d_output[index] = 0;
				else
					d_output[index] = 1;
			}
		}
	}

	if (blockIdx.x == 0 && tid == 0)
		d_output[d_par[0].start] = 0;
}

/**
 * wrapper function of thrust key-value sort utility
 *
 * sort entries according to d_keys
 *
 */
void gpu_sort(uint32 *d_keys, uint32 *d_values, uint32 size)
{
/*
	b40c::radix_sort::Enactor enactor;
	b40c::util::DoubleBuffer<uint32, uint32> sort_storage(d_keys, d_values);
	enactor.Sort(sort_storage, size);
*/
	
	thrust::device_ptr<uint32> d_key_ptr = thrust::device_pointer_cast(d_keys);
	thrust::device_ptr<uint32> d_value_ptr = thrust::device_pointer_cast(d_values);

	thrust::sort_by_key(d_key_ptr, d_key_ptr+size, d_value_ptr);

	HANDLE_ERROR(cudaDeviceSynchronize());
}

void gpu_block_sort(uint32 *d_keys, uint32 *d_values, uint32 start, uint32 len)
{
	thrust::device_ptr<uint32> d_key_ptr = thrust::device_pointer_cast(d_keys+start);
	thrust::device_ptr<uint32> d_value_ptr = thrust::device_pointer_cast(d_values+start);
	
	thrust::sort_by_key(d_key_ptr, d_key_ptr+len, d_value_ptr);
}


/**
 * the function update_block takes prefix sum result as input
 * d_wp should store the last element of each group
 */
bool update_block_init(uint32 *d_sa, uint32 *d_isa, uint32 *d_input, uint32 *d_wp, uint32 *d_value, 
	uint32 *d_len, uint32 *d_misa, Partition *d_par, Partition *h_par, uint32 *h_block_start, uint32 *h_block_len, 
	uint32 par_count, uint32 num_unique, uint32& bsort_boundary, uint32 &gt_one_bound, uint32 &s_type_bound, 
	uint32 &l_type_bound, uint32 string_size, uint32 h)
{
	dim3 blocks_per_grid(1, 1, 1);
	dim3 threads_per_block(THREADS_PER_BLOCK, 1, 1);
	uint32 last = h_par[par_count-1].end;
	uint32 num_interval = par_count/BLOCK_NUM + (par_count%BLOCK_NUM?1:0);
	uint32 bound[4];

	update_block_kernel1_init<<<BLOCK_NUM, threads_per_block>>>(d_input, d_value, d_par, num_interval, par_count);
	CHECK_KERNEL_ERROR("update_block_kernel1_init");

	blocks_per_grid.x = num_unique/(THREADS_PER_BLOCK*NUM_ELEMENT_ST) + (num_unique%(THREADS_PER_BLOCK*NUM_ELEMENT_ST) ? 1 : 0);
	mem_host2device(&last, d_value+num_unique+1, sizeof(uint32));
	update_block_kernel2_init<<<blocks_per_grid, threads_per_block>>>(d_len, d_value, num_unique);		
	CHECK_KERNEL_ERROR("update_block_kernel2_init");
	
	gpu_sort(d_len, d_value, num_unique+1);

	/*
	 * boundary info is stored in d_value[num_unique]
	 */
	find_boundary_kernel_init<<<blocks_per_grid, threads_per_block>>>(d_len, num_unique);
	CHECK_KERNEL_ERROR("find_boundary_kernel_init");

	mem_device2host(d_len+num_unique+1, bound, sizeof(uint32)*4);

	bsort_boundary = bound[0];
	gt_one_bound = bound[1];
	s_type_bound = bound[2];
	l_type_bound = bound[3];

#ifdef __DEBUG__
	printf("bitonic sort boundary: %u\n", bsort_boundary);
	printf("greater than one boundary: %u\n", gt_one_bound);
	printf("S type upperbound: %u\n", s_type_bound);
	printf("L type upperbound: %u\n", l_type_bound);
#endif	
	if (gt_one_bound == 0)
		return true;

	blocks_per_grid.x = 256;
	threads_per_block.x = 256;

//	HANDLE_ERROR(cudaThreadSynchronize());

	Start(0);
//	uint32 round_string_size = (string_size/NUM_ELEMENT_SB + (string_size%NUM_ELEMENT_SB?1:0))*NUM_ELEMENT_SB;
//	uint32 global_num = round_string_size/h;
//	num_interval = round_string_size/NUM_ELEMENT_SB/blocks_per_grid.x;

#ifdef __DEBUG__	
//	printf("global num: %u, num of interval: %u\n", global_num, num_interval);
#endif

//	gather_module_based_isa_kernel<<<blocks_per_grid, threads_per_block>>>(d_isa, d_misa, h, num_interval, round_string_size, global_num);

#ifdef __DEBUG__
//	HANDLE_ERROR(cudaThreadSynchronize());
//	check_module_based_isa(d_isa, d_misa, h, string_size, global_num);
#endif
	

//	num_interval = (bsort_boundary-gt_one_bound)/blocks_per_grid.x + ((bsort_boundary-gt_one_bound)%blocks_per_grid.x?1:0);
//	bitonic_sort_kernel_init<<<blocks_per_grid, threads_per_block>>>(d_len+gt_one_bound, d_value+gt_one_bound, d_sa, d_misa, h, num_interval, bsort_boundary-gt_one_bound, h, global_num);

//	HANDLE_ERROR(cudaThreadSynchronize());
//	Stop(0);
	
	cpu_small_group_sort(d_sa, d_isa, d_len+gt_one_bound, d_value+gt_one_bound, bsort_boundary-gt_one_bound, string_size, h);

#ifdef __DEBUG__	
	HANDLE_ERROR(cudaThreadSynchronize());
	check_small_group_sort(d_sa, d_isa, d_len+gt_one_bound, d_value+gt_one_bound, bsort_boundary-gt_one_bound, string_size, h);
#endif 
			
	mem_device2host(d_value + bsort_boundary, h_block_start, sizeof(uint32)*(num_unique+1-bsort_boundary));
	mem_device2host(d_len + bsort_boundary, h_block_len, sizeof(uint32)*(num_unique+1-bsort_boundary));
	
	return false;
}


/**
 * the function update_block takes prefix sum result as input
 * d_wp should store the last element of each group
 */
bool update_block(uint32 *d_sa, uint32 *d_isa, uint32 *d_input, uint32 *d_wp, uint32 *d_value, 
	uint32 *d_len, uint32 *d_block_start_old, uint32 *d_block_len_old, uint32 *h_block_start, 
	uint32 *h_block_len, uint32 block_count, uint32 num_unique, uint32 &gt_zero_bound,
	uint32 &bsort_boundary, uint32 &gt_one_bound, uint32 &s_type_bound, uint32 &l_type_bound, uint32 string_size, uint32 h)
{
	dim3 blocks_per_grid(1, 1, 1);
	dim3 threads_per_block(THREADS_PER_BLOCK, 1, 1);
	uint32 num_interval;
	uint32 bound[4];
	
	HANDLE_ERROR(cudaMemset(d_len, 0, sizeof(uint32)*(num_unique+6)));

	update_block_kernel1<<<BLOCK_NUM, threads_per_block>>>(d_input, d_len, d_value, d_block_start_old, d_block_len_old, block_count);
	CHECK_KERNEL_ERROR("update_block_kernel1");

//	blocks_per_grid.x = num_unique/(THREADS_PER_BLOCK*NUM_ELEMENT_ST) + (num_unique%(THREADS_PER_BLOCK*NUM_ELEMENT_ST) ? 1 : 0);
//	update_block_kernel2<<<blocks_per_grid, threads_per_block>>>(d_len, d_value, num_unique);
//	CHECK_KERNEL_ERROR("update_block_kernel2");
	
	gpu_sort(d_len, d_value, num_unique+1);

#ifdef __DEBUG__
	uint32 _par_count = num_unique+1;
	uint32 *block_len = (uint32*)allocate_pageable_memory(sizeof(uint32) * (_par_count));
	uint32 *block_start = (uint32*)allocate_pageable_memory(sizeof(uint32) * (_par_count));
	mem_device2host(d_len, block_len, sizeof(uint32)*(_par_count));
	mem_device2host(d_value, block_start, sizeof(uint32)*(_par_count));
//	for (uint32 i = 0; i < _par_count; i++)
//		printf("block_len[%u]: %u, %u\n", i, block_len[i], block_start[i]);
	free_pageable_memory(block_len);
	free_pageable_memory(block_start);
#endif

	/*
	 * boundary info is stored in d_value[num_unique], if num_unique is very large, there may be some problem here
	 */
	blocks_per_grid.x = (num_unique+1)/(THREADS_PER_BLOCK*NUM_ELEMENT_ST) + ((num_unique+1)%(THREADS_PER_BLOCK*NUM_ELEMENT_ST) ? 1 : 0);
	find_boundary_kernel<<<blocks_per_grid, threads_per_block>>>(d_len, num_unique+1);
	CHECK_KERNEL_ERROR("find_boundary_kernel");

	mem_device2host(d_len+num_unique+1, bound, sizeof(uint32)*4);
	bsort_boundary = bound[0];
	gt_one_bound = bound[1];
	l_type_bound = bound[2];
	gt_zero_bound = bound[3];

#ifdef __DEBUG__
	printf("bitonic sort boundary: %u\n", bsort_boundary);
	printf("greater than one boundary: %u\n", gt_one_bound);
	printf("greater than zero boundary: %u\n", gt_zero_bound);
	printf("L type upperbound: %u\n", l_type_bound);
#endif
		
	if (gt_one_bound == 0)
		return true;

	if (bsort_boundary - gt_one_bound > 0)
	{
		blocks_per_grid.x = 256;
		threads_per_block.x = 256;
		num_interval = (bsort_boundary-gt_one_bound)/blocks_per_grid.x + ((bsort_boundary-gt_one_bound)%blocks_per_grid.x?1:0);
		
	//	HANDLE_ERROR(cudaThreadSynchronize());
	//	Start(0);

	//	bitonic_sort_kernel<<<blocks_per_grid, threads_per_block>>>(d_len+gt_one_bound, d_value+gt_one_bound, d_sa, d_isa, h, num_interval, bsort_boundary-gt_one_bound, string_size+5);

	//	HANDLE_ERROR(cudaThreadSynchronize());
	//	Stop(0);
		cpu_small_group_sort(d_sa, d_isa, d_len+gt_one_bound, d_value+gt_one_bound, bsort_boundary-gt_one_bound, string_size, h);

#ifdef __DEBUG__	
		HANDLE_ERROR(cudaThreadSynchronize());
		check_small_group_sort(d_sa, d_isa, d_len+gt_one_bound, d_value+gt_one_bound, bsort_boundary-gt_one_bound, string_size, h);
#endif
	}	
	
	if (bsort_boundary >= num_unique+1)
		return true;
		
	mem_device2host(d_value + bsort_boundary, h_block_start, sizeof(uint32)*(num_unique+1-bsort_boundary));
	mem_device2host(d_len + bsort_boundary, h_block_len, sizeof(uint32)*(num_unique+1-bsort_boundary));
	
	return false;
}

void scatter_rank_value(uint32 *d_block_len, uint32 *d_block_start, uint32 *d_sa, uint32 *d_isa, uint32 split_bound, uint32 par_count, uint32 string_size)
{
	dim3 blocks_per_grid(BLOCK_NUM, 1, 1);
	dim3 threads_per_block(THREADS_PER_BLOCK, 1, 1);

//	check_block_complete(d_block_len, d_block_start, d_sa, par_count, string_size);

	uint32 num_interval = split_bound/blocks_per_grid.x + (split_bound%blocks_per_grid.x?1:0);
	scatter_small_group_kernel<<<blocks_per_grid, threads_per_block>>>(d_block_len, d_block_start, d_sa, d_isa, num_interval, split_bound);
	CHECK_KERNEL_ERROR("scatter_small_group_kernel");

	blocks_per_grid.x = BLOCK_NUM/4;
	num_interval = (par_count-split_bound)/blocks_per_grid.x + ((par_count-split_bound)%blocks_per_grid.x?1:0);

#ifdef __DEBUG__
	printf("scatter: num_interval: %u\nsplit_bound: %u\npar_count: %u\n", num_interval, split_bound, par_count);
	
	uint32 *h_block_len = (uint32*)allocate_pageable_memory(sizeof(uint32) * (par_count-split_bound));
	uint32 *h_block_start = (uint32*)allocate_pageable_memory(sizeof(uint32) * (par_count-split_bound));
	mem_device2host(d_block_len+split_bound, h_block_len, sizeof(uint32) * (par_count-split_bound));
	mem_device2host(d_block_start+split_bound, h_block_start, sizeof(uint32) * (par_count-split_bound));
//	for (uint32 i = 0; i < par_count-split_bound; i++)
//		printf("%u: (%u, %u, %u)\n", i, h_block_len[i], h_block_start[i], h_block_len[i]+h_block_start[i]);
	free_pageable_memory(h_block_len);
	free_pageable_memory(h_block_start);
#endif
		
	scatter_large_group_kernel<<<blocks_per_grid, threads_per_block>>>(d_block_len+split_bound, d_block_start+split_bound, d_sa, d_isa, num_interval, par_count-split_bound);
	CHECK_KERNEL_ERROR("scatter_large_group_kernel");

	HANDLE_ERROR(cudaThreadSynchronize());
}

void bucket_result(uint32 *d_sa, uint32 *d_isa, uint8* h_ref, uint32 string_size)
{
	uint32 *h_sa = (uint32*)allocate_pageable_memory(sizeof(uint32) * string_size);
	uint32 *h_isa = (uint32*)allocate_pageable_memory(sizeof(uint32) * string_size);
	mem_device2host(d_sa, h_sa, sizeof(uint32) * string_size);
	mem_device2host(d_isa, h_isa, sizeof(uint32) * string_size);
	
	uint32 start_pos;

	for (uint32 i = 0; i < string_size; i++)
	{
		start_pos = h_sa[i];
		printf("bucket val: %#x, ref val: %#x%x%x%x\n", h_isa[i], h_ref[start_pos], h_ref[start_pos+1], h_ref[start_pos+2], h_ref[start_pos+3]);
	}

	free_pageable_memory(h_sa);
	free_pageable_memory(h_isa);
}


void scatter(uint32 *d_L, uint32 *d_R_in, uint32 *d_R_out, Partition *d_par, uint32 par_count)
{
	
	/* 
	 * my implementation of scatter
	 */
	
	dim3 threads_per_block(THREADS_PER_BLOCK, 1, 1);
	dim3 blocks_per_grid(1, 1, 1);
	
	uint32 shared_size = THREADS_PER_BLOCK * sizeof(uint32) * 8;
	blocks_per_grid.x = par_count;
	
	
	scatter_kernel<<<blocks_per_grid, threads_per_block, shared_size>>>(d_L, d_R_in, d_R_out, d_par);
	CHECK_KERNEL_ERROR("scatter_kernel");
	HANDLE_ERROR(cudaDeviceSynchronize());
	

	/*thurst scatter operation*/
/*		
	thrust::device_ptr<uint32> d_input_ptr = thrust::device_pointer_cast(d_R_in);
	thrust::device_ptr<uint32> d_output_ptr = thrust::device_pointer_cast(d_R_out);
	thrust::device_ptr<uint32> d_map_ptr = thrust::device_pointer_cast(d_L);

	thrust::scatter(d_input_ptr, d_input_ptr+size, d_map_ptr, d_output_ptr);
	cudaDeviceSynchronize();
*/	
}

uint32 prefix_sum(uint32 *d_input, uint32 *d_output, uint32 size)
{
	uint32 sum = 0;
	uint32 first_rank = 1;

	mem_host2device(&first_rank, d_input, sizeof(uint32));

	thrust::device_ptr<uint32> d_input_ptr = thrust::device_pointer_cast(d_input);
	thrust::device_ptr<uint32> d_output_ptr = thrust::device_pointer_cast(d_output);

	thrust::inclusive_scan(d_input_ptr, d_input_ptr+size, d_output_ptr);
	
	mem_device2host(d_output+size-1, &sum, sizeof(uint32));
	
	return sum;
}

uint32 block_prefix_sum(uint32 *d_input, uint32 *d_block_totals, Partition *d_par, Partition *h_par, uint32 par_count)
{
	uint32 inclusive = 1;
	uint32 total = 0;
	uint32 num_interval = par_count/BLOCK_NUM + (par_count%BLOCK_NUM?1:0); 

	BlockScanPass1<<<BLOCK_NUM, NUM_THREADS>>>(d_input, d_par, d_block_totals, num_interval, par_count);
	BlockScanPass2<<<1, NUM_THREADS>>>(d_block_totals, BLOCK_NUM);
	BlockScanPass3<<<BLOCK_NUM, NUM_THREADS>>>(d_input, d_par, d_block_totals, num_interval, par_count, inclusive);
	HANDLE_ERROR(cudaThreadSynchronize());

	mem_device2host(d_input+h_par[par_count-1].end-1, &total, sizeof(uint32));
	
	return total;
}

/**
 * Update isa and unsorted groups
 * the function update_isa() can handle at most 65536*2048 elements
 */
bool update_isa_block(uint32 *d_sa, uint32 *d_isa_out, uint32 *d_isa_in, 
	uint32 h_order, uint32 *&d_block_start, uint32 *&d_block_len, uint32 *&d_block_start_ano, uint32 *&d_block_len_ano, uint32 *d_ps_array, 
	uint32 *h_block_start, uint32 *h_block_len, Partition *d_par, Partition *h_par, 
	uint32 par_count, uint32 string_size, uint32 &block_count, uint32 &seg_sort_lower_bound, 
	uint32 &s_type_bound, uint32 &l_type_bound, uint8 *h_ref)
{
	dim3 threads_per_block(THREADS_PER_BLOCK, 1, 1);
	dim3 blocks_per_grid(1, 1, 1);
	

	uint32 num_unique = 0;
	uint32 gt_zero_bound = 0;
	uint32 split_boundary;
	uint32 sort_bound;
	uint32 shared_size = (THREADS_PER_BLOCK) * sizeof(uint32) * 16;
	uint32 num_interval = par_count/BLOCK_NUM + (par_count%BLOCK_NUM ? 1:0);
	blocks_per_grid.x = BLOCK_NUM;

#ifdef __MEASURE_TIME__
	Start(NEIG_COM);
#endif	

#ifdef __DEBUG__	
	printf("number of thread blocks: %d\n", par_count);
#endif
	/*
	 * input:  d_isa_out
	 * output: d_block_len
	 */
	neighbour_comparison_kernel<<<blocks_per_grid, threads_per_block, shared_size>>>(d_isa_out, d_ps_array, d_par, num_interval, par_count);
	CHECK_KERNEL_ERROR("neighbour_comparision_kernel");
	
//	neighbour_comparison_kernel2<<<blocks_per_grid, threads_per_block, shared_size>>>(d_isa_out, d_block_len, d_par, num_interval);
//	CHECK_KERNEL_ERROR("neighbour_comparision_kernel2");
	
	HANDLE_ERROR(cudaDeviceSynchronize());
#ifdef __DEBUG__	
	check_neighbour_comparison(d_isa_out, d_ps_array, h_par, par_count, string_size);
#endif

#ifdef __MEASURE_TIME__
	Stop(NEIG_COM);
	Start(PREFIX_SUM);
#endif	

#ifdef __DEBUG__
	/*
	 * test prefix sum result
	 */
	uint32 *h_input = (uint32*)allocate_pageable_memory(sizeof(uint32)*string_size);
	mem_device2host(d_ps_array, h_input, sizeof(uint32)*string_size);
#endif
	num_unique = block_prefix_sum(d_ps_array, d_block_start_ano, d_par, h_par, par_count);

#ifdef __DEBUG__
	printf("number of unique : %d\n", num_unique);
	
	check_prefix_sum(h_input, d_ps_array, h_par, par_count, string_size);
#endif

#ifdef __MEASURE_TIME__
	Stop(PREFIX_SUM);
#endif
	
	if (update_block(d_sa, d_isa_in, d_ps_array, d_block_len_ano, d_block_start_ano, d_block_len_ano, d_block_start+seg_sort_lower_bound, d_block_len+seg_sort_lower_bound, h_block_start, h_block_len, block_count, num_unique, gt_zero_bound, split_boundary, sort_bound, s_type_bound, l_type_bound, string_size, h_order))
		return true;

//	check_seg_isa(d_block_len_ano+gt_zero_bound, d_block_start_ano+gt_zero_bound, d_sa, num_unique+1-gt_zero_bound, h_ref, string_size, h_order);
//	check_update_block(d_block_len, d_block_start, h_input, par_count, h_par, string_size, split_boundary, sort_bound);

#ifdef __DEBUG__
	free_pageable_memory(h_input);
#endif	

#ifdef __MEASURE_TIME__
	Start(SCATTER);
#endif
	scatter_rank_value(d_block_len_ano+gt_zero_bound, d_block_start_ano+gt_zero_bound, d_sa, d_isa_in, split_boundary-gt_zero_bound, num_unique+1-gt_zero_bound, string_size);
	block_count = (num_unique+1)-split_boundary;
#ifdef __MEASURE_TIME__
	Stop(SCATTER);
#endif	
	seg_sort_lower_bound = split_boundary;
	::swap(d_block_len, d_block_len_ano);
	::swap(d_block_start, d_block_start_ano);
	return false;
}


/**
 * Update isa and unsorted groups
 * the function update_isa() can handle at most 65536*2048 elements
 */
bool update_isa_block_init(uint32 *d_sa, uint32 *d_isa_out, uint32 *d_isa_in, 
	uint32 h_order, uint32 *d_block_start, uint32 *d_block_len, uint32 *h_block_start, 
	uint32 *h_block_len, Partition *d_par, Partition *h_par, 
	uint32 par_count, uint32 string_size, uint32 &new_par_count, uint32 &seg_sort_lower_bound, 
	uint32 &s_type_bound, uint32 &l_type_bound)
{
	dim3 threads_per_block(THREADS_PER_BLOCK, 1, 1);
	dim3 blocks_per_grid(1, 1, 1);
	
	uint32 num_unique = 0;
	uint32 split_boundary;
	uint32 sort_bound;
	uint32 shared_size = (THREADS_PER_BLOCK) * sizeof(uint32) * 16;
	uint32 num_interval = par_count/BLOCK_NUM + (par_count%BLOCK_NUM ? 1:0);
	blocks_per_grid.x = BLOCK_NUM;

#ifdef __MEASURE_TIME__
	Start(NEIG_COM);
#endif	

#ifdef __DEBUG__	
	printf("number of thread blocks: %d\n", par_count);
#endif

	/*
	 * input:  d_isa_out
	 * output: d_block_len
	 */
	neighbour_comparison_kernel<<<blocks_per_grid, threads_per_block, shared_size>>>(d_isa_out, d_block_len, d_par, num_interval, par_count);
	CHECK_KERNEL_ERROR("neighbour_comparision_kernel");
	
//	neighbour_comparison_kernel2<<<blocks_per_grid, threads_per_block, shared_size>>>(d_isa_out, d_block_len, d_par, num_interval);
//	CHECK_KERNEL_ERROR("neighbour_comparision_kernel2");
	
	HANDLE_ERROR(cudaDeviceSynchronize());

#ifdef __DEBUG__	
	check_neighbour_comparison(d_isa_out, d_block_len, h_par, par_count, string_size);
#endif

#ifdef __MEASURE_TIME__
	Stop(NEIG_COM);
	Start(PREFIX_SUM);
#endif	
#ifdef __DEBUG__
	/*
	 * test prefix sum result
	 */
	uint32 *h_input = (uint32*)allocate_pageable_memory(sizeof(uint32)*string_size);
	mem_device2host(d_block_len, h_input, sizeof(uint32)*string_size);
#endif
	num_unique = block_prefix_sum(d_block_len, d_block_start, d_par, h_par, par_count);

#ifdef __DEBUG__
	printf("number of unique : %d\n", num_unique);
	
	check_prefix_sum(h_input, d_block_len, h_par, par_count, string_size);
#endif

#ifdef __MEASURE_TIME__
	Stop(PREFIX_SUM);
#endif
	
	//to be modified
	if (update_block_init(d_sa, d_isa_in, d_block_len, d_block_len, d_block_start, d_block_len, d_isa_out,
	d_par, h_par, h_block_start, h_block_len, par_count, num_unique, 
	split_boundary, sort_bound, s_type_bound, l_type_bound, string_size, h_order))
		return true;
	
//	check_update_block(d_block_len, d_block_start, h_input, par_count, h_par, string_size, split_boundary, sort_bound);

#ifdef __DEBUG__
	free_pageable_memory(h_input);
#endif	

#ifdef __MEASURE_TIME__
	Start(SCATTER);
#endif
	scatter_rank_value(d_block_len, d_block_start, d_sa, d_isa_in, split_boundary, num_unique+1, string_size);
	new_par_count = (num_unique+1)-split_boundary;
#ifdef __MEASURE_TIME__
	Stop(SCATTER);
#endif	
	seg_sort_lower_bound = split_boundary;
	HANDLE_ERROR(cudaThreadSynchronize());
	return false;
}


//to be modified later
void derive_2h_order(uint32 *d_sa, uint32 *d_isa_in, uint32 *d_isa_out, uint32 h_order, Partition *h_par, Partition *d_par,
		uint32 *d_block_start, uint32 *d_block_len, uint32 *h_block_start, uint32 *h_block_len, uint32 *d_digits, uint32 *d_tmp_store, uint32 par_count, 
		uint32 block_count, uint32 seg_sort_lower_bound, uint32 s_type_bound, uint32 l_type_bound, uint32 s_type_par_bound, uint32 l_type_par_bound, uint32 digit_count)
{
	dim3 threads_per_block(THREADS_PER_BLOCK, 1, 1);
	dim3 blocks_per_grid(BLOCK_NUM, 1, 1);
	
	uint32 shared_size = (THREADS_PER_BLOCK) * sizeof(uint32) * 8; 
	uint32 num_interval = par_count/BLOCK_NUM + (par_count%BLOCK_NUM?1:0);
	
	//gather operation
	//to be modified later
	get_second_keys<<<blocks_per_grid, threads_per_block, shared_size>>>(d_sa, d_isa_in+h_order, d_isa_out, d_par, num_interval, par_count);
	CHECK_KERNEL_ERROR("get_second_keys");

#ifdef __MEASURE_TIME__
	Start(R_SORT);
#endif

#ifdef __DEBUG__

	printf("par_count: %u\n", par_count);
	printf("block_count: %u\n", block_count);
	if (s_type_bound)
		printf("s_type_bound: %u, length: %u %u\n", s_type_bound, h_block_len[s_type_bound-1], h_block_len[s_type_bound]);
	if (l_type_bound)
		printf("l_type_bound: %u, length: %u %u\n", l_type_bound, h_block_len[l_type_bound-1], h_block_len[l_type_bound]);
//	printf("s_type_par_bound: %u %u %u\n", s_type_par_bound, h_block_len[h_par[s_type_par_bound-1].bid-1], h_block_len[h_par[s_type_par_bound].bid-1]);
//	printf("l_type_par_bound: %u %u %u\n", l_type_par_bound, h_block_len[h_par[l_type_par_bound-1].bid-1], h_block_len[h_par[l_type_par_bound].bid-1]);
//	printf("l_type_par_bound: %u %u\n", l_type_par_bound, h_par[l_type_par_bound-1].bid);
	
#endif	
	uint32 num_thread = NUM_THREAD_SEG_SORT;
	uint32 num_block;
	//L-type segment key-value sort
	uint32 l_block_count = l_type_bound > s_type_bound ? l_type_bound - s_type_bound : 0;
	if (l_block_count)
	{
		uint32 *d_block_start_ptr = d_block_start + s_type_bound + seg_sort_lower_bound;
		uint32 *d_block_len_ptr = d_block_len + s_type_bound + seg_sort_lower_bound;
		Partition *d_par_ptr = d_par + s_type_par_bound;
		uint32 num_block = l_block_count < NUM_BLOCK_SEG_SORT ? l_block_count : NUM_BLOCK_SEG_SORT;
		uint32 work_per_block = l_block_count/num_block + (l_block_count%num_block?1:0);
		uint32 num_interval_for_pass2 = work_per_block/NUM_WARPS + (work_per_block%NUM_WARPS?1:0);
	
		for (uint32 bit = 0; bit < 30; bit += 5)
		{
			HANDLE_ERROR(cudaMemset(d_digits, 0, digit_count));
			multiblock_radixsort_pass1<<<num_block, num_thread>>>(d_isa_out, d_digits+32, d_block_start_ptr, d_block_len_ptr, bit, l_block_count);
			multiblock_radixsort_pass2<<<num_block, num_thread>>>(d_digits+32, d_block_len_ptr, num_interval_for_pass2, l_block_count);
			multiblock_radixsort_pass3<<<num_block, num_thread>>>(d_digits+32, d_isa_out, d_sa, d_block_start_ptr, d_block_len_ptr, d_tmp_store, bit, l_block_count);
		}
	}
	if (s_type_bound)
	{
		//S-type segment key-value sort 
		uint s_par_count = s_type_par_bound;
		num_block = s_par_count < NUM_BLOCK_SEG_SORT ? s_par_count: NUM_BLOCK_SEG_SORT;
		num_interval = s_par_count/num_block + (s_par_count%num_block?1:0);
	
		for (uint32 bit = 0; bit < 30; bit +=5)
		//TODO: to be modified later
			single_block_radixsort<<<num_block, num_thread>>>(d_isa_out, d_sa, d_block_start+seg_sort_lower_bound, d_block_len+seg_sort_lower_bound, num_interval, bit, s_par_count);
	}
	HANDLE_ERROR(cudaDeviceSynchronize());

	//At current stage, we call thrust for segments longer than 65535
	for (uint32 i = l_type_bound; i < block_count; i++)
		gpu_block_sort(d_isa_out, d_sa, h_block_start[i], h_block_len[i]);

#ifdef __MEASURE_TIME__
	Stop(R_SORT);
#endif
}

void assign_thread_blocks(Partition *h_par, Partition *d_par, uint32 *h_block_start, uint32 *h_block_len, uint32 block_count, uint32 &par_count, uint32 &s_type_bound, uint32 &l_type_bound,
			uint32 &s_type_par_bound, uint32 &l_type_par_bound)
{
	par_count = 0;
	uint32 start, end;
	uint32 pre_parcount;
	
	s_type_bound = 0;
	l_type_bound = 0;
	s_type_par_bound = 0;
	l_type_par_bound = 0;

	for (uint32 i = 0; i < block_count; i++)
	{
		if (h_block_len[i] < NUM_ELEMENT_SB)
		{
			h_par[par_count].bid = i+1;
			h_par[par_count].start = h_block_start[i];
			h_par[par_count].end = h_block_start[i] + h_block_len[i];
			par_count++;
		}
		else
		{
			start = h_block_start[i];
			end = start + h_block_len[i];
			pre_parcount = par_count;
			for (uint32 j = start; j < end; j += NUM_ELEMENT_SB)
			{
				h_par[par_count].bid = 0;
				h_par[par_count].start = j;
				h_par[par_count].end = j+NUM_ELEMENT_SB;
				par_count++;
			}
			h_par[pre_parcount].bid = i+1;
			h_par[par_count-1].end = end;
		}

		if (h_block_len[i] <= NUM_ELEMENT_SB && (i == block_count-1 || h_block_len[i+1] > NUM_ELEMENT_SB))
		{	
			s_type_bound = i+1;
			s_type_par_bound = par_count;
		}
		if (h_block_len[i] <= MAX_SEG_NUM && (i == block_count-1 || h_block_len[i+1] > MAX_SEG_NUM))
		{	
			l_type_bound = i+1;
			l_type_par_bound = par_count;
		}
	}
	mem_host2device(h_par, d_par, sizeof(Partition)*par_count);
}

/*
 * release version
 */

/*
 * debug version
 * include correctness checking function
 */
void gpu_suffix_sorting_debug(uint32* h_sa, uint32* h_ref, uint32 string_size)
{
	dim3 threads_per_block(THREADS_PER_BLOCK, 1, 1);
	dim3 blocks_per_grid(1, 1, 1);
	
	uint32 ch_per_uint32 = 4;
	uint32 shared_size = (THREADS_PER_BLOCK+4) *4; 
	uint32 size_d_ref = string_size/ch_per_uint32+1;
	uint32 h_order = ch_per_uint32;
	uint32 block_count;
	uint32 par_count;
	uint32 seg_sort_lower_bound;
	uint32 s_type_bound;
	uint32 l_type_bound;
	uint32 s_type_par_bound;
	uint32 l_type_par_bound;
	uint32 cpu_round_pow2[MIN_UNSORTED_GROUP_SIZE+2];

	uint32* h_isa = (uint32*)allocate_pageable_memory(sizeof(uint32) * string_size);
	uint32* d_ref = (uint32*)allocate_device_memory(sizeof(uint32) * size_d_ref);
	uint32* d_sa = (uint32*)allocate_device_memory(sizeof(uint32) * string_size);
	uint32* d_isa_in = (uint32*)allocate_device_memory_roundup(sizeof(uint32) * string_size, sizeof(uint32)*NUM_ELEMENT_SB);
	uint32* d_isa_out = (uint32*)allocate_device_memory_roundup(sizeof(uint32) * string_size, sizeof(uint32)*NUM_ELEMENT_SB);
	blocks_per_grid.x = CEIL(CEIL(string_size, ch_per_uint32), threads_per_block.x);
		
	uint32* h_block_start = (uint32*)allocate_pageable_memory(sizeof(uint32) * (string_size));
	uint32* h_block_len = (uint32*)allocate_pageable_memory(sizeof(uint32) * (string_size));
	uint32* d_block_start = (uint32*)allocate_device_memory(sizeof(uint32) * (string_size));
	uint32* d_block_len = (uint32*)allocate_device_memory(sizeof(uint32) * (string_size));
	uint32* d_block_start_ano = (uint32*)allocate_device_memory(sizeof(uint32) * (string_size));
	uint32* d_block_len_ano = (uint32*)allocate_device_memory(sizeof(uint32) * (string_size));
	uint32* d_ps_array = (uint32*)allocate_device_memory_roundup(sizeof(uint32) * (string_size), sizeof(uint32)*(NUM_ELEMENT_SB));

// 	Partition* h_par = (Partition*)allocate_pageable_memory(sizeof(Partition) * (string_size/NUM_ELEMENT_SB+32));
//	Partition* d_par = (Partition*)allocate_device_memory(sizeof(Partition) * (string_size/NUM_ELEMENT_SB+32));

 	Partition* h_par = (Partition*)allocate_pageable_memory(sizeof(Partition) * (string_size/MIN_UNSORTED_GROUP_SIZE+32));
	Partition* d_par = (Partition*)allocate_device_memory(sizeof(Partition) * (string_size/MIN_UNSORTED_GROUP_SIZE+32));

	//allocate memory for segmented sort
	uint32 digit_count = sizeof(uint32)*16*NUM_LIMIT*32;
	uint32 *d_digits = (uint32*)allocate_device_memory(digit_count);
	uint32 *d_tmp_store = (uint32*)allocate_device_memory(sizeof(uint32) * NUM_BLOCK_SEG_SORT * MAX_SEG_NUM *2);

	/* shared memory configuration*/
	enum cudaFuncCache pCacheConfig;
	HANDLE_ERROR(cudaDeviceSetCacheConfig(cudaFuncCachePreferShared));
	HANDLE_ERROR(cudaDeviceGetCacheConfig(&pCacheConfig));

	if (pCacheConfig == cudaFuncCachePreferNone)
		printf("cache perference: none \n");
	else if (pCacheConfig == cudaFuncCachePreferShared)
		printf("cache perference: shared memory \n");
	else if (pCacheConfig == cudaFuncCachePreferL1)
		printf("cache perference: L1 cache \n");
	else
		printf("cache perference: error\n");
	
	gpu_mem_usage();
	mem_host2device(h_ref, d_ref, sizeof(uint32) * size_d_ref);
	generate_bucket_with_shift<<<blocks_per_grid, threads_per_block, shared_size >>>(d_sa, d_ref, d_isa_in, string_size);
	CHECK_KERNEL_ERROR("generate_bucket_with_shift");
	HANDLE_ERROR(cudaDeviceSynchronize());

	/* initialize block information*/
	h_block_start[0] = 0;
	h_block_len[0] = string_size;
	block_count = 1;
	
	/* initialize round_pow2*/
//	init_round_pow2(cpu_round_pow2);
//	HANDLE_ERROR(cudaMemcpyToSymbol("round_pow2", cpu_round_pow2, sizeof(uint32)*(MIN_UNSORTED_GROUP_SIZE+2), 0, cudaMemcpyHostToDevice));
	
	Setup(0);

	assign_thread_blocks(h_par, d_par, h_block_start, h_block_len, block_count, par_count, s_type_bound, l_type_bound, s_type_par_bound, l_type_par_bound);
	
	mem_device2device(d_isa_in, d_isa_out, sizeof(uint32) * string_size);

	::swap(d_isa_in, d_isa_out); // seems useless
	
	/* initialzie max value used by bitonic sort */
	uint32 max_val = 0xffffffff;
	mem_host2device(&max_val, d_isa_in+string_size+5, sizeof(uint32));

	/* sort bucket index stored in d_isa_in*/
	gpu_block_sort(d_isa_out, d_sa, h_block_start[0], h_block_len[0]);

#ifdef __DEBUG__	
	check_h_order_correctness(d_sa, (uint8*)h_ref, string_size, h_order);
#endif
	
	update_isa_block_init(d_sa, d_isa_out, d_isa_in, h_order, d_block_start, d_block_len, h_block_start, h_block_len, d_par, h_par, par_count, string_size, block_count, seg_sort_lower_bound, s_type_bound, l_type_bound);

#ifdef __DEBUG__
	check_isa(d_sa, d_isa_in, (uint8*)h_ref, string_size, h_order);
#endif
		
	for (; h_order < string_size; h_order *= 2)
	{
		
		printf("--------------------iteration %u------------------------\n--------------------------------------------------------\n", h_order*2);
		/* 
		 * besides assigning thread blocks, 
		 * the following function will copy partition information to the device,
		 * this part will be removed later
		 */
		assign_thread_blocks(h_par, d_par, h_block_start, h_block_len, block_count, par_count, s_type_bound, l_type_bound, s_type_par_bound, l_type_par_bound);
		
		derive_2h_order(d_sa, d_isa_in, d_isa_out, h_order, h_par, d_par, d_block_start, d_block_len, h_block_start, h_block_len, d_digits, d_tmp_store, par_count, block_count, seg_sort_lower_bound, s_type_bound, l_type_bound, s_type_par_bound, l_type_par_bound, digit_count);
		
#ifdef __DEBUG__	
		check_h_order_correctness(d_sa, (uint8*)h_ref, string_size, h_order*2);
#endif

		//update isa and unsorted group
		if(update_isa_block(d_sa, d_isa_out, d_isa_in, h_order, d_block_start, d_block_len, d_block_start_ano, d_block_len_ano, d_ps_array, h_block_start, h_block_len, d_par, h_par, par_count, string_size, block_count, seg_sort_lower_bound, s_type_bound, l_type_bound, (uint8*)h_ref))
			break;
#ifdef __DEBUG__		
		check_isa(d_sa, d_isa_in, (uint8*)h_ref, string_size, 2*h_order);
#endif	
	}

#ifdef __DEBUG__
	check_h_order_correctness(d_sa, (uint8*)h_ref, string_size, string_size);
#endif
	//transfer output to host memory
	mem_device2host(d_sa, h_sa, sizeof(uint32) * string_size);
	
	printf("time cost for bitonic sort in init iteration: %.2f ms\n", GetElapsedTime(0)*1000);

	//free dvice memory
	free_device_memory(d_ref);
	free_device_memory(d_sa);
	free_device_memory(d_isa_in);
	free_device_memory(d_isa_out);
	free_device_memory(d_block_start);
	free_device_memory(d_block_len);
	free_device_memory(d_par);
	free_device_memory(d_digits);
	free_device_memory(d_tmp_store);
	free_device_memory(d_block_start_ano);
	free_device_memory(d_block_len_ano);
	free_device_memory(d_ps_array);
	
	//free host memory
	free_pageable_memory(h_isa);
	free_pageable_memory(h_block_start);
	free_pageable_memory(h_block_len);
	free_pageable_memory(h_par);
	
	HANDLE_ERROR(cudaDeviceReset());
}
